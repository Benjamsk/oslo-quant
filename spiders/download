#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import argparse
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
import multiprocessing

from osloquant.spiders.netfonds import NetfondsSpider
from osloquant.spiders.nasdaqomx import NasdaqOmxSpider

parser = argparse.ArgumentParser(description="Download market data")
parser.add_argument("--all", action="store_true", help="Download from all markets")
parser.add_argument("--oslobors", action="store_true", help="Download from Oslo BÃ¸rs")
parser.add_argument("--nasdaqomx", action="store_true", help="Download from NasdaqOMX")
args = parser.parse_args()

def run_splash():
    """This function will start the splash server in a new process"""

    # used for blocking the main thread until the splash server is up
    semaphore = multiprocessing.Semaphore(value=0)
    
    # this function will be called when the splash server is up and running
    def splash_started(opts, stderr):
        semaphore.release()

    from splash.server import main
    import splash
    
    # monkey patch this function to call this local function when the server starts
    splash.server.splash_started = splash_started

    # this is a hack to avoid ReactorAlreadyInstalledError
    if 'twisted.internet.reactor' in sys.modules:
        del sys.modules['twisted.internet.reactor']

    # start the splash server in a new process
    proc = multiprocessing.Process(target=main,
                                   kwargs={'argv': sys.argv[:1]},
                                   daemon=True)
    proc.start()

    # wait until the splash server is up and running
    semaphore.acquire()

    print("Splash server started")
        

def download_oslobors():
    process = CrawlerProcess(get_project_settings())
    process.crawl(NetfondsSpider)
    process.start()

def download_nasdaqomx():

    # this spider requires the splash server to be running
    # because the page contains dynamic content generated by javascript
    run_splash()

    process = CrawlerProcess(get_project_settings())
    process.crawl(NasdaqOmxSpider)
    process.start()

# if no arguments were given
if len(sys.argv) == 1:
    parser.print_help()

if args.all or args.oslobors:
    download_oslobors()

if args.all or args.nasdaqomx:
    download_nasdaqomx()
